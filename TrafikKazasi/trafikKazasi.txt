import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, roc_curve, auc
from statsmodels.tsa.arima.model import ARIMA
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from datetime import timedelta, datetime
import warnings

# DeprecationWarning'i gizle
warnings.filterwarnings("ignore", category=DeprecationWarning)

# 1. Excel Dosyasını Yükleyin ve Veriyi Temizleyin
file_path = r'C:\kazalar1\kaza_saat_farklari.xlsx'
df = pd.read_excel(file_path)

# 'fark' Kolonunu Sayısal Hale Getirme
def convert_to_minutes(fark):
    match = re.match(r"(\d+)\s*saat,\s*(\d+)\s*dakika", fark)
    if match:
        hours = int(match.group(1))
        minutes = int(match.group(2))
        return hours * 60 + minutes
    else:
        return np.nan

df['fark_mikro'] = df['fark'].apply(convert_to_minutes)

# NaN olanları temizleyelim
df_clean = df.dropna(subset=['fark_mikro'])

# Tarihi sıralı hale getirme
df_clean = df_clean.sort_values(by='kazatarihi')

# 2. Ek Özellikler Ekleyin
def sin_cos_transform(data, period):
    return np.sin(2 * np.pi * data / period), np.cos(2 * np.pi * data / period)

df_clean['hour_sin'], df_clean['hour_cos'] = sin_cos_transform(df_clean['kazatarihi'].dt.hour, 24)
df_clean['dayofweek_sin'], df_clean['dayofweek_cos'] = sin_cos_transform(df_clean['kazatarihi'].dt.dayofweek, 7)
df_clean['month_sin'], df_clean['month_cos'] = sin_cos_transform(df_clean['kazatarihi'].dt.month, 12)

# Özellikleri ve hedef değişkeni hazırlayın
features = df_clean[['fark_mikro', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos']].values
target = df_clean['fark_mikro'].values

# 3. Logaritmik Dönüşüm ve Normalize Etme
def log_transform(data):
    return np.log1p(data)

target = log_transform(target)
scaler_x = MinMaxScaler(feature_range=(0, 1))
scaler_y = MinMaxScaler(feature_range=(0, 1))
features_scaled = scaler_x.fit_transform(features)
target_scaled = scaler_y.fit_transform(target.reshape(-1, 1))

# 4. Rolling Window Kullanarak Özellikler ve Etiketler
def create_dataset(data, target, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i + window_size].flatten())  # Pencereyi düzleştiriyoruz
        y.append(target[i + window_size])
    return np.array(X), np.array(y)

window_size = 30  # Pencere boyutu
X, y = create_dataset(features_scaled, target_scaled, window_size)

# 5. Eğitim ve Test Setlerini Bölme
split_index = int(len(X) * 0.8)  # %80 eğitim, %20 test
X_train, y_train = X[:split_index], y[:split_index]
X_test, y_test = X[split_index:], y[split_index:]

# ARIMA Modeli ile Tahmin
arima_model = ARIMA(df_clean['fark_mikro'], order=(5, 1, 0))
arima_model_fit = arima_model.fit()
arima_forecast = arima_model_fit.forecast(steps=len(y_test))  # Test seti boyutunda tahmin

# LSTM Modeli
X_train_lstm = X_train.reshape(X_train.shape[0], window_size, -1)
X_test_lstm = X_test.reshape(X_test.shape[0], window_size, -1)

lstm_model = Sequential()
lstm_model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))
lstm_model.add(Dropout(0.2))
lstm_model.add(LSTM(30, activation='relu', return_sequences=True))
lstm_model.add(Dropout(0.2))
lstm_model.add(LSTM(20, activation='relu'))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer='adam', loss='mse')

lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_lstm, y_test), verbose=1)

y_pred_lstm = lstm_model.predict(X_test_lstm)
y_pred_lstm_original = scaler_y.inverse_transform(y_pred_lstm)

# Ensemble (LSTM ve ARIMA Birleşimi)
arima_forecast_original = scaler_y.inverse_transform(np.array(arima_forecast).reshape(-1, 1))
alpha = 0.7  # LSTM ağırlığı
beta = 0.3   # ARIMA ağırlığı
final_prediction = (alpha * y_pred_lstm_original.flatten() + beta * arima_forecast_original.flatten())

# Tahmin edilen bir sonraki kaza tarihi
geri_ölçekli_son_kaza = scaler_y.inverse_transform(target_scaled[-1].reshape(-1, 1))[0][0]
bir_sonraki_fark = np.expm1(geri_ölçekli_son_kaza)
son_kaza_tarihi = df_clean.iloc[-1]['kazatarihi']
son_kaza_tarih_saat = pd.to_datetime(f"{son_kaza_tarihi}")
bir_sonraki_kaza_tarih_saat = son_kaza_tarih_saat + timedelta(minutes=bir_sonraki_fark)

# Tahmin doğruluğunu hesaplama
gercek_tarih = datetime.strptime("2024-12-16 08:54:00", "%Y-%m-%d %H:%M:%S")
tahmin_tarih = bir_sonraki_kaza_tarih_saat
fark = (gercek_tarih - tahmin_tarih).total_seconds()

gercek_fark_saniye = (gercek_tarih - datetime(2024, 12, 16, 0, 0, 0)).total_seconds()
dogruluk = (1 - abs(fark) / gercek_fark_saniye) * 100

# Çıktı
print(f"\nSon kaza tarihi ve saati: {son_kaza_tarih_saat.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Tahmin edilen bir sonraki kaza tarihi ve saati: {bir_sonraki_kaza_tarih_saat.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"\nTahmin doğruluğu: %{dogruluk:.2f}")

# ROC Eğrisi ve AUC Hesaplama
threshold = np.median(scaler_y.inverse_transform(y_test))
y_test_binary = (scaler_y.inverse_transform(y_test) > threshold).astype(int).flatten()
y_pred_binary = (final_prediction > threshold).astype(int).flatten()

fpr, tpr, _ = roc_curve(y_test_binary, y_pred_binary)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

# Ek Grafikler
# Gerçek ve tahmin değerlerinin karşılaştırması
plt.figure(figsize=(10, 6))
plt.scatter(scaler_y.inverse_transform(y_test), final_prediction, alpha=0.7, edgecolors='k')
plt.xlabel("Gerçek Değerler")
plt.ylabel("Tahmin Edilen Değerler")
plt.title("Gerçek vs Tahmin Edilen Değerler")
plt.grid()
plt.show()

# Hata dağılım grafiği
errors = scaler_y.inverse_transform(y_test) - final_prediction
plt.figure(figsize=(10, 6))
plt.hist(errors, bins=20, edgecolor='k', alpha=0.7)
plt.title("Hata Dağılımı")
plt.xlabel("Hata")
plt.ylabel("Frekans")
plt.grid()
plt.show()

# Tahmin ve Gerçek Zaman Farkı
time_differences = [abs((gercek_tarih - bir_sonraki_kaza_tarih_saat).total_seconds()) / 60 for gercek_tarih in df_clean['kazatarihi'][-len(final_prediction):]]
plt.figure(figsize=(12, 6))
plt.plot(time_differences, label='Tahmin ve Gerçek Zaman Farkı (Dakika)')
plt.title("Tahmin ve Gerçek Zaman Farkı")
plt.xlabel("Örnek")
plt.ylabel("Fark (Dakika)")
plt.grid()
plt.legend()
plt.show()